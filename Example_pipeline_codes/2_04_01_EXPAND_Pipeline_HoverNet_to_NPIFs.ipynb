{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e4f55d-65bd-41cf-8de1-d445ba455667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### --------------------------------------------------------------------------------------\n",
    "#### author: Ranjan Barman, date: Aug 22, 2025\n",
    "#### Final job submission code: Steps 2→4 (HoVer-Net → Morphology → NPIFs)\n",
    "#### --------------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import time\n",
    "import subprocess\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# =========================\n",
    "# Static paths (edit if needed)\n",
    "# =========================\n",
    "_wpath_       = \"/data/Lab_ruppin/Ranjan/HnE/\"\n",
    "dataset_name  = \"TCGA_BRCA_FFPE\"\n",
    "tiles_path    = \"/data/Ruppin_AI/Datasets/TCGA_BRCA_FFPE/outputs/tiles/\"\n",
    "\n",
    "# Your exact .py codes (update paths if different)\n",
    "step2_code_path = \"/data/Ruppin_AI/BRCA_PIF/Ranjan/Codes/EXPAND_Pipeline/\"\n",
    "step2_code_file = \"2_01_22_ExtractMorphologicalFeaturesFromHnE.py\"\n",
    "\n",
    "step3_code_path = \"/data/Ruppin_AI/BRCA_PIF/Ranjan/Codes/EXPAND_Pipeline/\"\n",
    "step3_code_file = \"2_02_03_MorphologyCalculation_All_Slides_Pipeline.py\"  # expects: -slide <SLIDE_NAME> and --out_dir\n",
    "\n",
    "step4_code_path = \"/data/Ruppin_AI/BRCA_PIF/Ranjan/Codes/EXPAND_Pipeline/\"\n",
    "step4_code_file = \"2_03_01_01_NPIFs_Calculation_HoverNet_Pipeline.py\"     # computes Top-25% internally\n",
    "\n",
    "# Parameters (kept for reference)\n",
    "mpp_value = 0.248\n",
    "\n",
    "# SLURM resources\n",
    "gpu_partition = \"gpu\"\n",
    "gpu_gres      = \"gpu:k80:2,lscratch:20\"\n",
    "gpu_cpus      = 8\n",
    "gpu_mem       = \"64g\"\n",
    "gpu_time      = \"36:00:00\"   # Step 2\n",
    "\n",
    "cpu_partition = \"norm\"\n",
    "cpu_cpus      = 8\n",
    "cpu_mem       = \"32g\"\n",
    "cpu_time_s3   = \"06:00:00\"   # Step 3\n",
    "cpu_time_s4   = \"04:00:00\"   # Step 4\n",
    "\n",
    "# Environment\n",
    "conda_activate = \"source ~/miniconda3/etc/profile.d/conda.sh && conda activate hovernet\"\n",
    "module_lines   = [\"module load CUDA/10.2\", \"module load gcc/13.2.0\"]\n",
    "\n",
    "# =========================\n",
    "# CLI\n",
    "# =========================\n",
    "os.chdir(_wpath_)\n",
    "parser = ArgumentParser(description=\"Submit SLURM jobs for Steps 2→4 (HoVer-Net→Morph→NPIFs) with strict dependencies.\")\n",
    "parser.add_argument(\"-run\",   type=str, default=\"n\", help=\"Submit jobs? [y/n]\")\n",
    "parser.add_argument(\"-date\",  type=str, default=date.today().strftime(\"%d%b%Y\"), help=\"Date stamp (e.g., 22Jan2025)\")\n",
    "parser.add_argument(\"-delay\", type=int, default=5, help=\"Delay (sec) between job submissions\")\n",
    "parser.add_argument(\"-trial\", type=int, default=1, help=\"Trial index for job folder naming\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "submit_jobs      = (args.run.lower() == \"y\")\n",
    "datestamp        = args.date\n",
    "submission_delay = args.delay\n",
    "trial            = args.trial\n",
    "\n",
    "# =========================\n",
    "# Slide selection\n",
    "# =========================\n",
    "sample_prefixes = [\"TCGA-AC-A23H\", \"TCGA-BH-A1EN\", \"TCGA-A2-A04X\"]\n",
    "\n",
    "all_files     = os.listdir(tiles_path)\n",
    "matched_files = sorted({f for p in sample_prefixes for f in all_files if fnmatch.fnmatch(f, f\"{p}*tiles.bz2\")})\n",
    "\n",
    "print(\"Matched Files:\")\n",
    "for f in matched_files:\n",
    "    print(\" \", f)\n",
    "\n",
    "if not matched_files:\n",
    "    raise FileNotFoundError(\"No matching *.tiles.bz2 found in the tiles directory.\")\n",
    "\n",
    "# =========================\n",
    "# Job/Log directories\n",
    "# =========================\n",
    "job_path = f\"{_wpath_}{dataset_name}/outputs/HoverNet/jobs/{datestamp}_{trial}/\"\n",
    "log_path = f\"{_wpath_}{dataset_name}/outputs/HoverNet/jobs/logs/{datestamp}_{trial}/\"\n",
    "os.makedirs(job_path, exist_ok=True)\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# Derived output roots for later steps\n",
    "morph_root = f\"{_wpath_}{dataset_name}/outputs/Morph/\"\n",
    "npifs_root = f\"{_wpath_}{dataset_name}/outputs/NPIFs/\"\n",
    "os.makedirs(morph_root, exist_ok=True)\n",
    "os.makedirs(npifs_root, exist_ok=True)\n",
    "\n",
    "# Helpers\n",
    "def sbatch(path, dependency=None):\n",
    "    cmd = [\"sbatch\"]\n",
    "    if dependency:\n",
    "        cmd += [f\"--dependency=afterok:{dependency}\"]\n",
    "    cmd += [path]\n",
    "    out = subprocess.check_output(cmd, text=True).strip()\n",
    "    print(out)\n",
    "    # \"Submitted batch job <id>\"\n",
    "    return out.split()[-1]\n",
    "\n",
    "# =========================\n",
    "# Write & submit Step 2 (GPU) and Step 3 (CPU) per slide with dependency\n",
    "# =========================\n",
    "s3_job_ids = []   # collect all Step-3 jobids for final Step-4 dependency\n",
    "\n",
    "for idx, slide_file in enumerate(tqdm(matched_files), start=1):\n",
    "    slide_name = os.path.splitext(slide_file)[0]\n",
    "\n",
    "    # ---- Step 2 (GPU)\n",
    "    s2_jobid = None  # CHANGED: define before use so we can test below\n",
    "    s2_script_path = os.path.join(job_path, f\"s2_{idx}_{slide_name}.sh\")\n",
    "    with open(s2_script_path, \"w\") as f:\n",
    "        f.writelines([\n",
    "            \"#!/bin/bash\\n\",\n",
    "            f\"#SBATCH --partition={gpu_partition}\\n\",\n",
    "            f\"#SBATCH --gres={gpu_gres}\\n\",\n",
    "            f\"#SBATCH --cpus-per-task={gpu_cpus}\\n\",\n",
    "            f\"#SBATCH --mem={gpu_mem}\\n\",\n",
    "            f\"#SBATCH --time={gpu_time}\\n\",\n",
    "            f\"#SBATCH --job-name=s2_{slide_name}\\n\",\n",
    "            f\"#SBATCH --output={log_path}s2_{idx}_{slide_name}.out\\n\",\n",
    "            f\"#SBATCH --error={log_path}s2_{idx}_{slide_name}.err\\n\",\n",
    "            \"\\n\",\n",
    "            \"set -euo pipefail\\n\",\n",
    "            f\"{conda_activate}\\n\",\n",
    "            *(line + \"\\n\" for line in module_lines),\n",
    "            \"\\n\",\n",
    "            f\"python {os.path.join(step2_code_path, step2_code_file)} \"\n",
    "            f\"-slide {slide_file} -tile_path {tiles_path} -wd {_wpath_}\\n\",\n",
    "            # optional completion flag\n",
    "            f\"mkdir -p {_wpath_}{dataset_name}/outputs/HoverNet/{slide_name}/masks\\n\",\n",
    "            f\"touch {_wpath_}{dataset_name}/outputs/HoverNet/{slide_name}/masks/_PRED.done\\n\",\n",
    "        ])\n",
    "    if submit_jobs:\n",
    "        s2_jobid = sbatch(s2_script_path)\n",
    "        time.sleep(submission_delay)\n",
    "\n",
    "    # ---- Guard for resume-only runs (do NOT check during fresh submission)\n",
    "    pred_done = f\"{_wpath_}{dataset_name}/outputs/HoverNet/{slide_name}/masks/_PRED.done\"\n",
    "    if not submit_jobs and not os.path.exists(pred_done):  # CHANGED: only guard when not submitting\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing Step-2 flag: {pred_done}. Make sure Hover-Net finished for slide: {slide_name}\"\n",
    "        )\n",
    "\n",
    "    # ---- Step 3 (CPU)  (write to Morph root directly)\n",
    "    morph_out_dir = f\"{morph_root}{slide_name}/\"\n",
    "    masks_dir     = f\"{_wpath_}{dataset_name}/outputs/HoverNet/{slide_name}/masks\"\n",
    "\n",
    "    s3_script_path = os.path.join(job_path, f\"s3_{idx}_{slide_name}.sh\")\n",
    "    with open(s3_script_path, \"w\") as f:\n",
    "        f.writelines([\n",
    "            \"#!/bin/bash\\n\",\n",
    "            f\"#SBATCH --partition={cpu_partition}\\n\",\n",
    "            f\"#SBATCH --cpus-per-task={cpu_cpus}\\n\",\n",
    "            f\"#SBATCH --mem={cpu_mem}\\n\",\n",
    "            f\"#SBATCH --time={cpu_time_s3}\\n\",\n",
    "            f\"#SBATCH --job-name=s3_{slide_name}\\n\",\n",
    "            f\"#SBATCH --output={log_path}s3_{idx}_{slide_name}.out\\n\",\n",
    "            f\"#SBATCH --error={log_path}s3_{idx}_{slide_name}.err\\n\",\n",
    "            \"\\n\",\n",
    "            \"set -euo pipefail\\n\",\n",
    "            f\"{conda_activate}\\n\",\n",
    "            \"module load gcc/13.2.0\\n\",  # CPU-only; avoids Lmod swapping python with CUDA\n",
    "            \"\\n\",\n",
    "            f\"mkdir -p {morph_out_dir}\\n\",\n",
    "            # Step-3 writes CSV to Morph/<slide>/tumor_nuclei.csv\n",
    "            f\"python {os.path.join(step3_code_path, step3_code_file)} -slide {slide_name} --out_dir {morph_root}\\n\",\n",
    "        ])\n",
    "\n",
    "    if submit_jobs:\n",
    "        # CHANGED: depend on Step-2 so we don't need to check for flag here\n",
    "        s3_jobid = sbatch(s3_script_path, dependency=s2_jobid)\n",
    "        s3_job_ids.append(s3_jobid)\n",
    "        time.sleep(submission_delay)\n",
    "\n",
    "# =========================\n",
    "# Write & submit Step 4 (CPU) after ALL Step-3 jobs complete\n",
    "# =========================\n",
    "npifs_25q_csv = os.path.join(npifs_root, \"TCGA_BRCA_HoverNet_NPIFs_25Q.csv\")\n",
    "\n",
    "s4_script_path = os.path.join(job_path, \"s4_npifs_25q.sh\")  # renamed file for clarity\n",
    "with open(s4_script_path, \"w\") as f:\n",
    "    f.writelines([\n",
    "        \"#!/bin/bash\\n\",\n",
    "        f\"#SBATCH --partition={cpu_partition}\\n\",\n",
    "        f\"#SBATCH --cpus-per-task={cpu_cpus}\\n\",\n",
    "        f\"#SBATCH --mem={cpu_mem}\\n\",\n",
    "        f\"#SBATCH --time={cpu_time_s4}\\n\",\n",
    "        f\"#SBATCH --job-name=s4_npifs\\n\",\n",
    "        f\"#SBATCH --output={log_path}s4_npifs.out\\n\",\n",
    "        f\"#SBATCH --error={log_path}s4_npifs.err\\n\",\n",
    "        \"\\n\",\n",
    "        \"set -euo pipefail\\n\",\n",
    "        f\"{conda_activate}\\n\",\n",
    "        \"module load gcc/13.2.0\\n\",  # CPU-only\n",
    "        \"\\n\",\n",
    "        \"# ---------- NPIFs from TOP-25% tiles (default in the script) ----------\\n\",\n",
    "        f\"python {os.path.join(step4_code_path, step4_code_file)} \"\n",
    "        f\"--morph_root {morph_root} --output_csv {npifs_25q_csv}\\n\",\n",
    "    ])\n",
    "\n",
    "if submit_jobs:\n",
    "    if s3_job_ids:\n",
    "        dep_all = \":\".join(s3_job_ids)\n",
    "        out = subprocess.check_output([\"sbatch\", f\"--dependency=afterok:{dep_all}\", s4_script_path], text=True).strip()\n",
    "        print(out)\n",
    "    else:\n",
    "        print(f\"Would submit: sbatch --dependency=afterok:<s3_job_ids> {s4_script_path}\")\n",
    "\n",
    "print(\"Job submission completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe2f913-2719-4d29-88cf-1607e8c97a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 1.6.0\n",
      "CUDA version: 10.2\n",
      "GPU: Tesla K80 is available.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # Check GPU and CUDA compatibility\n",
    "# def check_gpu():\n",
    "#     if not torch.cuda.is_available():\n",
    "#         raise EnvironmentError(\"No GPU detected. Ensure CUDA and NVIDIA drivers are properly installed.\")\n",
    "#     print(f\"PyTorch version: {torch.__version__}\")\n",
    "#     print(f\"CUDA version: {torch.version.cuda}\")\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "\n",
    "# # Set device for inference\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Call the GPU check function\n",
    "# check_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724ab5e-f67b-4bd6-a011-ac87cbc29fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (hovernet)",
   "language": "python",
   "name": "hovernet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
